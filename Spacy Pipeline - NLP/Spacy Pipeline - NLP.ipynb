{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40cf906",
   "metadata": {},
   "source": [
    "## NLP - Spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16eac0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13761ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank pipeline in spacy\n",
    "nlp1 = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19cae384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do\n",
      "n't\n",
      "think\n",
      "just\n",
      "do\n",
      "it\n",
      ".\n",
      "This\n",
      "was\n",
      "said\n",
      "by\n",
      "goose\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp1(\"Don't think just do it. This was said by goose.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6322b8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we can see that there is no pipeline in the spacy.blank\n",
    "nlp1.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a4d2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#By running this code the error will pop up that in the pipeline there is no required tool.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentence)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\jkutty\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:923\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "#By running this code the error will pop up that in the pipeline there is no required tool.\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846cf76",
   "metadata": {},
   "source": [
    "### Adding a component to a blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0892cd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1d61e036710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp1.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a98401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't think just do it.\n",
      "This was said by goose.\n"
     ]
    }
   ],
   "source": [
    "#This will work because we add the necessary pipeline which is sentencizer\n",
    "doc = nlp1(\"Don't think just do it. This was said by goose.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda1fc3",
   "metadata": {},
   "source": [
    "### Spacy load pipeline(fully loaded pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a14a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fa398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Don't think just do it. This was said by goose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc6660e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1d61e31b830>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1d61e3560f0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1d6200017e0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1d61d9a7c10>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1d61e4a7350>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1d620001380>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126ee7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't think just do it.\n",
      "This was said by goose.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c77a90c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do | auxiliary | do\n",
      "n't | particle | not\n",
      "think | verb | think\n",
      "just | adverb | just\n",
      "do | verb | do\n",
      "it | pronoun | it\n",
      ". | punctuation | .\n",
      "This | pronoun | this\n",
      "was | auxiliary | be\n",
      "said | verb | say\n",
      "by | adposition | by\n",
      "goose | noun | goose\n",
      ". | punctuation | .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,'|',spacy.explain(token.pos_),'|',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ee56e",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4febb805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "#Detecting the name and labeled entity from the text eg : Tata an Organisation\n",
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for token in doc.ents:\n",
    "    print(token.text,token.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
