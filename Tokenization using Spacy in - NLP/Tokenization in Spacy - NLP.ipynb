{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8319ca3",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784e63d",
   "metadata": {},
   "source": [
    "Tokenization means splitting the text into smaller characters is called Tokenization.\n",
    "- Sentence Tokenization means splitting the paragraph into sentences.\n",
    "- Word tokenization means splitting the sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d8a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "862e8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing english as Language for spacy \n",
    "nlp = spacy.blank(\"en\") \n",
    "nlp2 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c307e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Dr.Strange is the one of the strongest avenger in MCU. He has magical powers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a865d5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "is\n",
      "the\n",
      "one\n",
      "of\n",
      "the\n",
      "strongest\n",
      "avenger\n",
      "in\n",
      "MCU\n",
      ".\n",
      "He\n",
      "has\n",
      "magical\n",
      "powers\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# The splitted words are called tokens\n",
    "for word in doc:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c65e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp2(\"Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e684d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens are the building blocks of Natural Language.\n",
      "Tokenization is a way of separating a piece of text into smaller units called tokens.\n",
      "Here, tokens can be either words, characters, or subwords.\n",
      "Hence, tokenization can be broadly classified into 3 types – word, character\n"
     ]
    }
   ],
   "source": [
    "#Splitting the paragraph into sentences\n",
    "for sentence in doc2.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c8a31c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens\n",
      "are\n",
      "the\n",
      "building\n",
      "blocks\n",
      "of\n",
      "Natural\n",
      "Language\n",
      ".\n",
      "Tokenization\n",
      "is\n",
      "a\n",
      "way\n",
      "of\n",
      "separating\n",
      "a\n",
      "piece\n",
      "of\n",
      "text\n",
      "into\n",
      "smaller\n",
      "units\n",
      "called\n",
      "tokens\n",
      ".\n",
      "Here\n",
      ",\n",
      "tokens\n",
      "can\n",
      "be\n",
      "either\n",
      "words\n",
      ",\n",
      "characters\n",
      ",\n",
      "or\n",
      "subwords\n",
      ".\n",
      "Hence\n",
      ",\n",
      "tokenization\n",
      "can\n",
      "be\n",
      "broadly\n",
      "classified\n",
      "into\n",
      "3\n",
      "types\n",
      "–\n",
      "word\n",
      ",\n",
      "character\n"
     ]
    }
   ],
   "source": [
    "#Splitting the paragraph into words\n",
    "for word in doc2:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116043dc",
   "metadata": {},
   "source": [
    "### Using indexes to Extract tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bfb6430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = doc2[0]\n",
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7957f159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc2[-1]\n",
    "token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd0d00d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2985b1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "048444de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f3249a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this we used spacy.blank(\"en\") is used so it is having only tokenizer as the pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36962c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this we used spacy.load(\"en_core_web_sm\") is used so it is having all the things in the pipeline\n",
    "nlp2.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e107b511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a7b5c",
   "metadata": {},
   "source": [
    "### Span Object\n",
    "\n",
    "Span Object means the slice from the doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4f17d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the building blocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token5 = doc2[2:5]\n",
    "print(token5)\n",
    "type(token5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db825918",
   "metadata": {},
   "source": [
    "### Token Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de0c3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp2(\"I have borrowed 500 $ from my friend strange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f82ee92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "220b014f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It shows the index of that particluar token\n",
    "token0.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f96d6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This means whether the text consist of alphabetic characters\n",
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58cda9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97bf4158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = doc[3]\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a3c6d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7cf77db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok4 = doc[4]\n",
    "tok4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5be41814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok4.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffe48b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "have ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "borrowed ==> index:  2 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "500 ==> index:  3 is_alpha: False is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  4 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "from ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "my ==> index:  6 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "friend ==> index:  7 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "strange ==> index:  8 is_alpha: True is_punct: False like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99234c80",
   "metadata": {},
   "source": [
    "### Collect email id from the students list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ad64a3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\\\nlp-text files\\\\students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fa500de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "17dee2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dayton high school, 8th grade students information\n",
       " \n",
       " Name\tbirth day   \temail\n",
       " -----\t------------\t------\n",
       " Virat   5 June, 1882    virat@kohli.com\n",
       " Maria\t12 April, 2001  maria@sharapova.com\n",
       " Serena  24 June, 1998   serena@williams.com \n",
       " Joe      1 May, 1997    joe@root.com\n",
       " \n",
       " \n",
       " "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "efce4490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "email = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        email.append(token.text)\n",
    "email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c3922",
   "metadata": {},
   "source": [
    "### Customizing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d68acf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d2ac36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp3 = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp3(\"gimme double cheese extra large healthy pizza\")\n",
    "k = [token.text for token in doc]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad2c32b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"},\n",
    "])\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318a2a3",
   "metadata": {},
   "source": [
    "### Questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a597eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the url from the text\n",
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8faf69d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Look for data to help you address the question. Governments are good\n",
       "sources because data from public research is often freely available. Good\n",
       "places to start include http://www.data.gov/, and http://www.science.\n",
       "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
       "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
       "and the European Social Survey at http://www.europeansocialsurvey.org/."
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp3(text)\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5f37df09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = []\n",
    "for token in doc2:\n",
    "    if token.like_url:\n",
    "        email.append(token.text)\n",
    "\n",
    "email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4394c881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony gave two $ to Peter, Bruce gave 500 € to Steve"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the price number and currency from the text \n",
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "doc3 = nlp3(transactions)\n",
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f9bd64e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two', '$', '500', '€']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currency = []\n",
    "for token in doc3:\n",
    "    if token.like_num or token.is_currency:\n",
    "        currency.append(token.text)\n",
    "currency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
